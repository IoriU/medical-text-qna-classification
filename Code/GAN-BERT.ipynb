{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3504fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 31 15:53:36 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.156.00   Driver Version: 450.156.00   CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-SXM4-40GB      On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    90W / 400W |  40380MiB / 40537MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-SXM4-40GB      On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    94W / 400W |  39450MiB / 40537MiB |     79%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-SXM4-40GB      On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   35C    P0   101W / 400W |  35120MiB / 40537MiB |     18%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-SXM4-40GB      On   | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    64W / 400W |  39003MiB / 40537MiB |     13%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-SXM4-40GB      On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   50C    P0    90W / 400W |  38809MiB / 40537MiB |     69%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  A100-SXM4-40GB      On   | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    88W / 400W |  39805MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  A100-SXM4-40GB      On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    56W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  A100-SXM4-40GB      On   | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    57W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647093fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7696616c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532d6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq transformers\n",
    "!pip install -qq torchsummary\n",
    "!pip install -qq scikit-multilearn\n",
    "!pip install -qq tqdm\n",
    "!pip install -qq ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156655bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", 'This pattern has match groups')\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ecfc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fac85ce6730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_val = 1906350912\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82233be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9940a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tersedia sejumlah 1 GPU(s).\n",
      "GPU yang akan digunakan: A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Tersedia sejumlah %d GPU(s).' % torch.cuda.device_count())\n",
    "    print('GPU yang akan digunakan:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('Tidak mendukung GPU; hanya CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf634609",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"Dataset/Human_Annotated.csv\", index_col=\"ID\")\n",
    "df_train.fillna(\"\", inplace = True)\n",
    "df_test = pd.read_csv(f\"Dataset/Gold_Standard.csv\", index_col=\"ID\")\n",
    "df_test.fillna(\"\", inplace = True)\n",
    "categories = ['Anak', 'Bedah', 'Gigi', 'Gizi', 'Jantung', 'Jiwa',\n",
    "       'Kandungan', 'Kulit dan Kelamin', 'Mata', 'Paru', 'Penyakit Dalam',\n",
    "       'Saraf', 'THT', 'Tulang', 'Urologi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "177d8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_umumnt = df_train.drop(columns=[\"Umum\"])\n",
    "df_train_umumnt[\"COUNT CLASS\"] = df_train_umumnt.drop(columns=[\"JUDUL\", \"ISI\"]).sum(axis=1)\n",
    "\n",
    "df_train = df_train_umumnt[df_train_umumnt[\"COUNT CLASS\"] > 0].drop(columns=[\"COUNT CLASS\"])\n",
    "df_test['Count'] = df_test.drop(columns=['JUDUL', 'ISI']).values.sum(axis=1)\n",
    "df_test = df_test[df_test['Count'] <= 3].drop(columns=['Count'])\n",
    "df_test_umumnt = df_test.drop(columns=[\"Umum\"])\n",
    "df_test_umumnt[\"COUNT CLASS\"] = df_test_umumnt.drop(columns=[\"JUDUL\", \"ISI\"]).sum(axis=1)\n",
    "df_test = df_test_umumnt[df_test_umumnt[\"COUNT CLASS\"] > 0].drop(columns=[\"COUNT CLASS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153daa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = ['DS', 'AD', 'DH', 'TD', 'KD']\n",
    "for i in code:\n",
    "  df_train[i] = df_train.index.map(lambda x: 1 if x[:2] == i else 0)\n",
    "\n",
    "X = df_train[['JUDUL', 'ISI']].values\n",
    "y = df_train.drop(columns=['JUDUL', 'ISI']).values\n",
    "\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split(X, y,test_size = 0.2)\n",
    "y_train = y_train[:,:-5]\n",
    "y_val = y_val[:,:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcdf14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[[\"JUDUL\", \"ISI\"] + categories]\n",
    "X_test = df_test[['JUDUL', 'ISI']].values\n",
    "y_test = df_test.drop(columns=['JUDUL', 'ISI']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf6eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_machine = pd.read_csv(f\"Dataset/Mini_Machine_Annotated.csv\", index_col=\"ID\").drop(columns=['KW_COUNT'])\n",
    "df_unlabel = pd.read_csv(f\"Dataset/Mini_unlabeled.csv\", index_col=\"ID\").drop(columns=['KW_COUNT'])\n",
    "unlabeled_examples = list(df_machine.values) + list(df_unlabel.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870581f1",
   "metadata": {},
   "source": [
    "# Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7def44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "MAX_SEQ_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "NUM_HIDDEN_LAYERS_G = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "NUM_HIDDEN_LAYERS_D = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "NOISE_SIZE = 256\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "OUT_DROPOUT_RATE = 0.01\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "APPLY_BALANCE = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 1e-5\n",
    "learning_rate_generator = 1e-5\n",
    "epsilon = 1e-8\n",
    "REGULARIZATION = 0.02\n",
    "num_train_epochs = 10\n",
    "multi_gpu = False\n",
    "EPOCH = 100\n",
    "# Scheduler\n",
    "apply_scheduler = True\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "PATH = f\"Model/ganbert-dapt.pt\"\n",
    "MODEL_NAME = \"dapt-indonlu-medqna\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3c14c",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aced7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "677fdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Anak', 'Bedah', 'Gigi', 'Gizi', 'Jantung', 'Jiwa',\n",
    "       'Kandungan', 'Kulit dan Kelamin', 'Mata', 'Paru', 'Penyakit Dalam',\n",
    "       'Saraf', 'THT', 'Tulang', 'Urologi']\n",
    "label2idx = {k: v for v, k in enumerate(categories)}\n",
    "idx2label = {v: k for v, k in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27e61ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(texts, labels, masks, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for mask in masks:\n",
    "    if mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(texts)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for text, label, mask in zip(texts, labels, masks): \n",
    "    if mask == 1 or not balance_label_examples:\n",
    "      examples.append((text, label, mask))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if mask:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((text, label, mask))\n",
    "      else:\n",
    "        examples.append((text, label, mask))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_att_mask = []\n",
    "  input_label = []\n",
    "  input_mask = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label, mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text, \\\n",
    "                                  add_special_tokens = True, \\\n",
    "                                  max_length = MAX_SEQ_LENGTH, \\\n",
    "                                  padding = \"max_length\", \\\n",
    "                                  truncation = True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    input_att_mask.append([int(token_id > 0) for token_id in encoded_sent])\n",
    "    input_label.append(label)\n",
    "    input_mask.append(mask)\n",
    "    \n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_att_mask = torch.tensor(input_att_mask)\n",
    "  input_label = torch.tensor(input_label, dtype=torch.long)\n",
    "  input_mask = torch.tensor(input_mask)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_att_mask, input_label, input_mask)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = BATCH_SIZE) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c510aa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17353/4172711099.py:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:207.)\n",
      "  input_label = torch.tensor(input_label, dtype=torch.long)\n",
      "/tmp/ipykernel_17353/4172711099.py:55: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  input_mask = torch.tensor(input_mask)\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = X_train\n",
    "label_examples = y_train\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(train_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = list(train_examples) + list(unlabeled_examples)\n",
    "\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  tmp_labels = [[-1] * len(categories)] *len(unlabeled_examples)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "  label_examples = list(label_examples) + list(tmp_labels) \n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, label_examples, train_label_masks, do_shuffle = True, balance_label_examples = APPLY_BALANCE)\n",
    "\n",
    "# ------------------------------\n",
    "#   Load the test dataset\n",
    "# ------------------------------\n",
    "# The labeled (test) dataset is assigned with a mask set to True\n",
    "val_label_masks = np.ones(len(X_val), dtype=bool)\n",
    "val_dataloader = generate_data_loader(X_val, y_val, val_label_masks, do_shuffle = False, balance_label_examples = False)\n",
    "test_label_masks = np.ones(len(X_test), dtype=bool)\n",
    "test_dataloader = generate_data_loader(X_test, y_test, test_label_masks, do_shuffle = False, balance_label_examples = False)\n",
    "\n",
    "dataset = {\"Train\" : train_dataloader, \"Val\" : val_dataloader, \"Test\" : test_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7101365",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ee6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Generator as in \n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        hidden_sizes = [noise_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        output_rep = self.layers(noise)\n",
    "        return output_rep\n",
    "\n",
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return last_rep, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7c25df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(generator, (8, 100))\n",
    "# print()\n",
    "# summary(discriminator, (16, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f712a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = None\n",
    "generator = None\n",
    "discriminator = None\n",
    "def create_model(transformer, generator, discriminator):\n",
    "    if transformer != None:\n",
    "        del transformer, generator, discriminator\n",
    "        torch.cuda.empty_cache()\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    hidden_size = int(config.hidden_size)\n",
    "    # Define the number and width of hidden layers\n",
    "    hidden_levels_g = [hidden_size for i in range(0, NUM_HIDDEN_LAYERS_G)]\n",
    "    hidden_levels_d = [hidden_size for i in range(0, NUM_HIDDEN_LAYERS_D)]\n",
    "    \n",
    "    transformer = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    generator = Generator(noise_size=NOISE_SIZE, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=OUT_DROPOUT_RATE)\n",
    "    discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(categories), dropout_rate=OUT_DROPOUT_RATE)\n",
    "\n",
    "    if torch.cuda.is_available():    \n",
    "        generator.cuda()\n",
    "        discriminator.cuda()\n",
    "        transformer.cuda()\n",
    "    return transformer, generator, discriminator\n",
    "# transformer, generator, discriminator = create_model(transformer, generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91a6f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import copy\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score\n",
    "\n",
    "def get_fake_tensor(size):\n",
    "    return torch.tensor([[0] * len(categories) + [1]] * size).to(device)\n",
    "\n",
    "def train_model(transformers, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler, dis_scheduler, num_epochs=25, last_epoch=0, last_acc=0):\n",
    "  best_acc = last_acc\n",
    "  max_epoch = last_epoch + num_epochs\n",
    "  for epoch_i in range(last_epoch, max_epoch):\n",
    "    print(f\"Epoch {epoch_i + 1} / {max_epoch}\")\n",
    "    \n",
    "    # Change dataset\n",
    "    for phase in [\"Train\", \"Val\"]:\n",
    "      instance = dataset[phase]\n",
    "      if phase == \"Train\":\n",
    "        # Training variable\n",
    "        tr_g_loss = 0\n",
    "        tr_d_loss = 0 \n",
    "        tr_d_l_sup = 0\n",
    "        tr_d_l_unsup = 0\n",
    "        \n",
    "        transformer.train() #maybe redundant\n",
    "        discriminator.train()\n",
    "        generator.train()\n",
    "      else:\n",
    "        # validation variables\n",
    "        total_test_loss = 0\n",
    "\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        \n",
    "        transformer.eval() #maybe redundant\n",
    "        discriminator.eval()\n",
    "        generator.eval()\n",
    "        \n",
    "      # Untuk setiap batch di training data\n",
    "      pbar = tqdm(enumerate(instance), desc=phase, total=len(instance))\n",
    "      for step, batch in pbar:\n",
    "        \n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_input_att_mask = batch[1].to(device)\n",
    "        batch_label = batch[2].to(device)\n",
    "        batch_mask = batch[3].to(device)\n",
    "        real_batch_size = batch_input_ids.shape[0]\n",
    "        \n",
    "        if phase == \"Train\":\n",
    "            # Encode real data in the Transformer\n",
    "            model_outputs = transformer(batch_input_ids, attention_mask=batch_input_att_mask)\n",
    "            hidden_states = model_outputs[-1]\n",
    "            # Generate fake data that should have the same distribution of the ones\n",
    "            # encoded by the transformer. \n",
    "            # First noisy input are used in input to the Generator\n",
    "            noise = torch.zeros(real_batch_size, NOISE_SIZE, device=device).uniform_(0, 1)\n",
    "            # Gnerate Fake data\n",
    "            gen_rep = generator(noise)\n",
    "\n",
    "            # Generate the output of the Discriminator for real and fake data.\n",
    "            # First, we put together the output of the tranformer and the generator\n",
    "            disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
    "            # Then, we select the output of the disciminator\n",
    "            features, probs = discriminator(disciminator_input)\n",
    "\n",
    "            # Finally, we separate the discriminator's output for the real and fake\n",
    "            # data\n",
    "            features_list = torch.split(features, real_batch_size)\n",
    "            D_real_features = features_list[0]\n",
    "            D_fake_features = features_list[1]\n",
    "            probs_list = torch.split(probs, real_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "\n",
    "            #---------------------------------\n",
    "            #  LOSS evaluation\n",
    "            #---------------------------------\n",
    "            # Generator's LOSS estimation\n",
    "            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "#             g_loss_d = -torch.mean(torch.sum((1 - get_fake_tensor(D_fake_probs.size()[0])), dim = -1))\n",
    "            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "            g_loss = g_loss_d + g_feat_reg\n",
    "\n",
    "            # Disciminator's LOSS estimation\n",
    "            log_probs = D_real_probs[:,0:-1]\n",
    "            # The discriminator provides an output for labeled and unlabeled real data\n",
    "            # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "            per_example_loss = -torch.sum(batch_label * torch.log(log_probs) + (1 - batch_label) * torch.log(1 - log_probs), dim = -1)\n",
    "            per_example_loss = torch.masked_select(per_example_loss, batch_mask.to(device))\n",
    "            labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "            \n",
    "            # It may be the case that a batch does not contain labeled examples, \n",
    "            # so the \"supervised loss\" in this case is not evaluated\n",
    "            if labeled_example_count == 0:\n",
    "              D_L_Supervised = torch.tensor([0]).to(device)\n",
    "            else:\n",
    "              D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "            \n",
    "            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "#             D_L_unsupervised1U = -torch.mean(torch.sum(get_fake_tensor(D_real_probs.size()[0]) * torch.log(D_real_probs) + (1 - get_fake_tensor(D_real_probs.size()[0])) * torch.log(1 - D_real_probs), dim = -1))\n",
    "#             D_L_unsupervised2U = -torch.mean(torch.sum(get_fake_tensor(D_fake_probs.size()[0]) * torch.log(D_fake_probs)), dim = -1)\n",
    "            D_L_unsupervised = D_L_unsupervised1U + D_L_unsupervised2U\n",
    "            d_loss = D_L_Supervised + D_L_unsupervised\n",
    "            \n",
    "            #---------------------------------\n",
    "            #  OPTIMIZATION\n",
    "            #---------------------------------\n",
    "            # Avoid gradient accumulation\n",
    "            gen_optimizer.zero_grad()\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            # Calculate weigth updates\n",
    "            # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            d_loss.backward() \n",
    "\n",
    "            # Apply modifications\n",
    "            gen_optimizer.step()\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            # A detail log of the individual losses\n",
    "            #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "            #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "            #             g_loss_d, g_feat_reg))\n",
    "\n",
    "            # Save the losses to print them later\n",
    "            tr_g_loss += g_loss.item()\n",
    "            tr_d_loss += d_loss.item()\n",
    "            tr_d_l_sup += D_L_Supervised.item()\n",
    "            tr_d_l_unsup += D_L_unsupervised.item()\n",
    "\n",
    "            # Update the learning rate with the scheduler\n",
    "            if apply_scheduler:\n",
    "              gen_scheduler.step()\n",
    "              dis_scheduler.step()\n",
    "                \n",
    "            pbar.set_postfix(g_loss = str(f'{(tr_g_loss / len(instance)):.2f}'), d_loss = str(f'{(tr_d_loss / len(instance)):.2f}'), d_loss_sup = str(f'{(tr_d_l_sup / len(instance)):.2f}'), d_loss_unsup = str(f'{(tr_d_l_unsup / len(instance)):.2f}'))\n",
    "            sleep(0.1)\n",
    "\n",
    "        elif phase == \"Val\":\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                model_outputs = transformer(batch_input_ids, attention_mask=batch_input_att_mask)\n",
    "                hidden_states = model_outputs[-1]\n",
    "                _, probs = discriminator(hidden_states)\n",
    "                ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "                filtered_logits = probs[:,0:-1]\n",
    "                # Accumulate the test loss.\n",
    "                loss = -torch.mean(torch.sum(batch_label * torch.log(filtered_logits) + (1 - batch_label) * torch.log(1 - filtered_logits), dim = 1))\n",
    "                total_test_loss += loss\n",
    "\n",
    "            pred_l = []\n",
    "            for prob in filtered_logits:\n",
    "                pred = [1 if x >= 0.5 else 0 for x in prob]\n",
    "                if (np.sum(pred) == 0):\n",
    "                    _, idx = torch.max(prob, dim = 0)\n",
    "                    pred[idx] = 1\n",
    "                pred_l.append(pred)\n",
    "            pred_labels += pred_l\n",
    "            true_l = batch_label\n",
    "            true_labels += true_l.detach().cpu()\n",
    "            \n",
    "            # pbar.set_description(f\"Average loss : {(loss_track / len(instance)):.2f}\")\n",
    "            pbar.set_postfix(loss = str(f'{(total_test_loss / len(instance)):.2f}'))\n",
    "            sleep(0.1)\n",
    "            \n",
    "    # accuracy\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    true_labels = torch.stack(true_labels).numpy()\n",
    "    val_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    val_precision = precision_score(true_labels, pred_labels, average='micro')\n",
    "    val_recall = recall_score(true_labels, pred_labels, average='micro')\n",
    "    print(f\"Accuracy: {val_accuracy}, Precision (micro): {val_precision}, Recall (micro): {val_recall}\")\n",
    "\n",
    "    if (val_accuracy > best_acc):\n",
    "      \n",
    "      torch.save({\n",
    "                  'best_transformer_sd' : transformer.state_dict(),\n",
    "                  'best_generator_sd' : generator.state_dict(),\n",
    "                  'best_discriminator_sd' : discriminator.state_dict(),\n",
    "                  'best_gen_optimizer_sd' : gen_optimizer.state_dict(),\n",
    "                  'best_dis_optimizer_sd' : dis_optimizer.state_dict(),\n",
    "                  'best_gen_scheduler_sd' : gen_scheduler.state_dict(),\n",
    "                  'best_dis_scheduler_sd' : dis_scheduler.state_dict(),\n",
    "                  'best_epoch' : epoch_i,\n",
    "                  'best_acc' : val_accuracy\n",
    "                  }, PATH)\n",
    "      best_acc = val_accuracy\n",
    "\n",
    "\n",
    "  checkpoint = torch.load(PATH)\n",
    "  transformer.load_state_dict(checkpoint['best_transformer_sd'])\n",
    "  generator.load_state_dict(checkpoint['best_generator_sd'])\n",
    "  discriminator.load_state_dict(checkpoint['best_discriminator_sd'])\n",
    "  gen_optimizer.load_state_dict(checkpoint['best_gen_optimizer_sd'])\n",
    "  dis_optimizer.load_state_dict(checkpoint['best_dis_optimizer_sd'])\n",
    "  gen_scheduler.load_state_dict(checkpoint['best_gen_scheduler_sd'])\n",
    "  dis_scheduler.load_state_dict(checkpoint['best_dis_scheduler_sd'])\n",
    "  best_epoch = checkpoint['best_epoch']\n",
    "  best_acc = checkpoint['best_acc']\n",
    "\n",
    "  return {'transformer' : transformer, 'generator' : generator, 'discriminator' : discriminator, \\\n",
    "          'gen_optimizer' : gen_optimizer, 'dis_optimizer' : dis_optimizer,  \\\n",
    "          'gen_scheduler' : gen_scheduler, 'dis_scheduler' : dis_scheduler,  \\\n",
    "          'best_acc' : best_acc, \"best_epoch\" : best_epoch}  \n",
    "#   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0320bf2a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:09<00:00,  3.62it/s, d_loss=5.51, d_loss_sup=4.58, d_loss_unsup=0.92, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.25it/s, loss=3.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31309904153354634, Precision (micro): 0.38977635782747605, Recall (micro): 0.34173669467787116\n",
      "Epoch 2 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=2.81, d_loss_sup=2.06, d_loss_unsup=0.75, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.31it/s, loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6964856230031949, Precision (micro): 0.8146964856230032, Recall (micro): 0.7142857142857143\n",
      "Epoch 3 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=1.96, d_loss_sup=1.22, d_loss_unsup=0.73, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.30it/s, loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.731629392971246, Precision (micro): 0.8395061728395061, Recall (micro): 0.7619047619047619\n",
      "Epoch 4 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=1.52, d_loss_sup=0.79, d_loss_unsup=0.72, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.27it/s, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7348242811501597, Precision (micro): 0.8425925925925926, Recall (micro): 0.7647058823529411\n",
      "Epoch 5 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=1.33, d_loss_sup=0.61, d_loss_unsup=0.72, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.26it/s, loss=1.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7539936102236422, Precision (micro): 0.8473053892215568, Recall (micro): 0.7927170868347339\n",
      "Epoch 6 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=1.16, d_loss_sup=0.45, d_loss_unsup=0.72, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.24it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7539936102236422, Precision (micro): 0.8504398826979472, Recall (micro): 0.8123249299719888\n",
      "Epoch 7 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:09<00:00,  3.61it/s, d_loss=1.75, d_loss_sup=1.03, d_loss_unsup=0.72, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.26it/s, loss=3.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2747603833865815, Precision (micro): 0.3057324840764331, Recall (micro): 0.2689075630252101\n",
      "Epoch 8 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:09<00:00,  3.61it/s, d_loss=1.64, d_loss_sup=0.92, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.25it/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7635782747603834, Precision (micro): 0.8380681818181818, Recall (micro): 0.8263305322128851\n",
      "Epoch 9 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.94, d_loss_sup=0.22, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.23it/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7539936102236422, Precision (micro): 0.8419540229885057, Recall (micro): 0.8207282913165266\n",
      "Epoch 10 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.91, d_loss_sup=0.20, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.25it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7699680511182109, Precision (micro): 0.8559077809798271, Recall (micro): 0.8319327731092437\n",
      "Epoch 11 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.86, d_loss_sup=0.15, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.11it/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7603833865814696, Precision (micro): 0.8575581395348837, Recall (micro): 0.8263305322128851\n",
      "Epoch 12 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:11<00:00,  3.58it/s, d_loss=0.85, d_loss_sup=0.14, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.12it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7476038338658147, Precision (micro): 0.8323699421965318, Recall (micro): 0.8067226890756303\n",
      "Epoch 13 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.85, d_loss_sup=0.15, d_loss_unsup=0.71, g_loss=0.76]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.30it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7699680511182109, Precision (micro): 0.8467966573816156, Recall (micro): 0.8515406162464986\n",
      "Epoch 14 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:09<00:00,  3.62it/s, d_loss=0.81, d_loss_sup=0.11, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.26it/s, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7859424920127795, Precision (micro): 0.8575498575498576, Recall (micro): 0.8431372549019608\n",
      "Epoch 15 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.77, d_loss_sup=0.07, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.31it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7603833865814696, Precision (micro): 0.8424068767908309, Recall (micro): 0.8235294117647058\n",
      "Epoch 16 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.76, d_loss_sup=0.05, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7507987220447284, Precision (micro): 0.8352272727272727, Recall (micro): 0.8235294117647058\n",
      "Epoch 17 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:09<00:00,  3.62it/s, d_loss=0.79, d_loss_sup=0.09, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.24it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7667731629392971, Precision (micro): 0.8490028490028491, Recall (micro): 0.834733893557423\n",
      "Epoch 18 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.77, d_loss_sup=0.06, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.30it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7667731629392971, Precision (micro): 0.848314606741573, Recall (micro): 0.84593837535014\n",
      "Epoch 19 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:09<00:00,  3.61it/s, d_loss=0.76, d_loss_sup=0.05, d_loss_unsup=0.70, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.23it/s, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7284345047923323, Precision (micro): 0.8181818181818182, Recall (micro): 0.8067226890756303\n",
      "Epoch 20 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.73, d_loss_sup=0.03, d_loss_unsup=0.70, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.18it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7507987220447284, Precision (micro): 0.8389830508474576, Recall (micro): 0.8319327731092437\n",
      "Epoch 21 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.73, d_loss_sup=0.03, d_loss_unsup=0.70, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.23it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7731629392971247, Precision (micro): 0.8547008547008547, Recall (micro): 0.8403361344537815\n",
      "Epoch 22 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.77, d_loss_sup=0.07, d_loss_unsup=0.70, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.24it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7827476038338658, Precision (micro): 0.8554913294797688, Recall (micro): 0.8291316526610645\n",
      "Epoch 23 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.76, d_loss_sup=0.06, d_loss_unsup=0.70, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.23it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7699680511182109, Precision (micro): 0.848314606741573, Recall (micro): 0.84593837535014\n",
      "Epoch 24 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.59it/s, d_loss=0.75, d_loss_sup=0.05, d_loss_unsup=0.70, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.21it/s, loss=1.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7667731629392971, Precision (micro): 0.8513119533527697, Recall (micro): 0.8179271708683473\n",
      "Epoch 25 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:09<00:00,  3.61it/s, d_loss=0.76, d_loss_sup=0.05, d_loss_unsup=0.70, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.23it/s, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7891373801916933, Precision (micro): 0.8653295128939829, Recall (micro): 0.84593837535014\n",
      "Epoch 26 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.75, d_loss_sup=0.05, d_loss_unsup=0.70, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.27it/s, loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7539936102236422, Precision (micro): 0.8296089385474861, Recall (micro): 0.8319327731092437\n",
      "Epoch 27 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.73, d_loss_sup=0.03, d_loss_unsup=0.70, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.27it/s, loss=1.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7539936102236422, Precision (micro): 0.8352272727272727, Recall (micro): 0.8235294117647058\n",
      "Epoch 28 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.72, d_loss_sup=0.02, d_loss_unsup=0.70, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7539936102236422, Precision (micro): 0.8405797101449275, Recall (micro): 0.8123249299719888\n",
      "Epoch 29 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.72, d_loss_sup=0.02, d_loss_unsup=0.70, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s, loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.744408945686901, Precision (micro): 0.829971181556196, Recall (micro): 0.8067226890756303\n",
      "Epoch 30 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.78, d_loss_sup=0.08, d_loss_unsup=0.70, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s, loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7348242811501597, Precision (micro): 0.830028328611898, Recall (micro): 0.8207282913165266\n",
      "Epoch 31 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.73, d_loss_sup=0.03, d_loss_unsup=0.70, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.26it/s, loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7380191693290735, Precision (micro): 0.8267045454545454, Recall (micro): 0.8151260504201681\n",
      "Epoch 32 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.59it/s, d_loss=0.71, d_loss_sup=0.01, d_loss_unsup=0.70, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.18it/s, loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7412140575079872, Precision (micro): 0.8257142857142857, Recall (micro): 0.8095238095238095\n",
      "Epoch 33 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=0.72, d_loss_sup=0.02, d_loss_unsup=0.70, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.26it/s, loss=1.80]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7539936102236422, Precision (micro): 0.8362068965517241, Recall (micro): 0.8151260504201681\n",
      "Epoch 34 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.72, d_loss_sup=0.02, d_loss_unsup=0.70, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.29it/s, loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7412140575079872, Precision (micro): 0.830945558739255, Recall (micro): 0.8123249299719888\n",
      "Epoch 35 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.80, d_loss_sup=0.10, d_loss_unsup=0.70, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s, loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7380191693290735, Precision (micro): 0.8289855072463768, Recall (micro): 0.8011204481792717\n",
      "Epoch 36 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.73, d_loss_sup=0.03, d_loss_unsup=0.70, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.27it/s, loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7476038338658147, Precision (micro): 0.8450292397660819, Recall (micro): 0.8095238095238095\n",
      "Epoch 37 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.62it/s, d_loss=0.71, d_loss_sup=0.01, d_loss_unsup=0.70, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.27it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7412140575079872, Precision (micro): 0.8333333333333334, Recall (micro): 0.7983193277310925\n",
      "Epoch 38 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:08<00:00,  3.63it/s, d_loss=0.71, d_loss_sup=0.02, d_loss_unsup=0.70, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  7.32it/s, loss=1.80]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7412140575079872, Precision (micro): 0.8382352941176471, Recall (micro): 0.7983193277310925\n",
      "Epoch 39 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  91%|█████████▏| 824/901 [03:46<00:21,  3.63it/s, d_loss=0.65, d_loss_sup=0.01, d_loss_unsup=0.64, g_loss=0.65]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_50428/1213231886.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(transformers, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler, dis_scheduler, num_epochs, last_epoch, last_acc)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# retain_graph=True is required since the underlying graph will be deleted after backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# Apply modifications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformer, generator, discriminator = create_model(transformer, generator, discriminator)\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "num_train_instances = len(train_dataloader)\n",
    "num_train_steps = int(num_train_instances / BATCH_SIZE * EPOCH)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "#scheduler\n",
    "dis_scheduler = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "gen_scheduler = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "model = train_model(transformer, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler, dis_scheduler, num_epochs=EPOCH, last_epoch=0, last_acc=0)\n",
    "\n",
    "evaluate_dataset(transformer, generator, discriminator, dataset['Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "657cd7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.7891373801916933, Epoch : 24\n"
     ]
    }
   ],
   "source": [
    "info = torch.load('Model/ganbert-dapt.pt')\n",
    "# transformer.load_state_dict(info['best_transformer_sd'])\n",
    "# generator.load_state_dict(info['best_generator_sd'])\n",
    "# discriminator.load_state_dict(info['best_discriminator_sd'])\n",
    "print(f\"Acc : {info['best_acc']}, Epoch : {info['best_epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0d3f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, jaccard_score, \\\n",
    "hamming_loss, cohen_kappa_score, f1_score, precision_score, recall_score\n",
    "import math\n",
    "def eval_all(y_human, y_machine, categories):\n",
    "  print(\"================ Classification Report ================\")\n",
    "  print(classification_report(y_human, y_machine, target_names=categories))\n",
    "  print(\"================ Multi Label Score ================\")\n",
    "  eval_multi_label(y_human, y_machine, categories)\n",
    "\n",
    "def eval_multi_label(y_test, prediction, categories):\n",
    "    # Butuh informasi lebih lengkap? silakan disimak di bawah ini\n",
    "    print('Accuracy             :', accuracy_score(y_test, prediction))\n",
    "    print('Precision            :', precision_score(y_test, prediction, average='micro'))\n",
    "    print('Recall               :', recall_score(y_test, prediction, average='micro'))\n",
    "    print('F1 Score             :', f1_score(y_test, prediction, average='micro'))\n",
    "    print('Hamming Loss         :', hamming_loss(y_test, prediction))\n",
    "    print('Jaccard Score')\n",
    "    js = jaccard_score(y_test, prediction, average=None)\n",
    "    for i, j in zip(categories, js):\n",
    "      print(f\"{i:<20} : {j}\")\n",
    "    print('Jaccard Score Macro Average:', jaccard_score(y_test, prediction, average='macro'))\n",
    "    print('Jaccard Score Micro Average:', jaccard_score(y_test, prediction, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94d1d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(transformer, generator, discriminator, instance):\n",
    "    transformer.eval() \n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Untuk setiap batch di validation data\n",
    "    pbar = tqdm(enumerate(instance), desc=\"Evaluate\", total=len(instance))\n",
    "    for step, batch in pbar:\n",
    "\n",
    "        # Unpack sebuah batch dari DataLoader\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_input_att_mask = batch[1].to(device)\n",
    "        batch_label = batch[2].to(device)\n",
    "        batch_mask = batch[3].to(device)\n",
    "        real_batch_size = batch_input_ids.shape[0]\n",
    "\n",
    "        # jangan track gradient! ini sedang evaluasi, bukan training\n",
    "        with torch.no_grad():        \n",
    "            model_outputs = transformer(batch_input_ids, attention_mask=batch_input_att_mask)\n",
    "            hidden_states = model_outputs[-1]\n",
    "            _, probs = discriminator(hidden_states)\n",
    "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "            filtered_logits = probs[:,0:-1]\n",
    "            # Accumulate the test loss.\n",
    "\n",
    "        pred_l = []\n",
    "        for prob in filtered_logits:\n",
    "            pred = [1 if x >= 0.5 else 0 for x in prob]\n",
    "            if (np.sum(pred) == 0):\n",
    "                _, idx = torch.max(prob, dim = 0)\n",
    "                pred[idx] = 1\n",
    "            pred_l.append(pred)\n",
    "        pred_labels += pred_l\n",
    "        true_l = batch_label\n",
    "        true_labels += true_l.detach().cpu()\n",
    "\n",
    "\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    true_labels = torch.stack(true_labels).numpy()\n",
    "    eval_all(true_labels, pred_labels, categories)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2830cf84",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 40/40 [00:01<00:00, 31.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Classification Report ================\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "             Anak       0.95      0.95      0.95        37\n",
      "            Bedah       0.84      0.93      0.89        29\n",
      "             Gigi       1.00      0.86      0.92         7\n",
      "             Gizi       0.83      0.83      0.83        23\n",
      "          Jantung       0.40      0.29      0.33         7\n",
      "             Jiwa       0.82      0.82      0.82        22\n",
      "        Kandungan       0.91      0.87      0.89        61\n",
      "Kulit dan Kelamin       0.93      0.95      0.94        43\n",
      "             Mata       0.93      0.93      0.93        15\n",
      "             Paru       1.00      1.00      1.00         4\n",
      "   Penyakit Dalam       0.71      0.71      0.71        49\n",
      "            Saraf       0.75      0.50      0.60        12\n",
      "              THT       0.97      0.94      0.95        31\n",
      "           Tulang       0.82      1.00      0.90         9\n",
      "          Urologi       0.80      0.50      0.62         8\n",
      "\n",
      "        micro avg       0.87      0.85      0.86       357\n",
      "        macro avg       0.84      0.80      0.82       357\n",
      "     weighted avg       0.86      0.85      0.85       357\n",
      "      samples avg       0.88      0.87      0.87       357\n",
      "\n",
      "================ Multi Label Score ================\n",
      "Accuracy             : 0.7891373801916933\n",
      "Precision            : 0.8653295128939829\n",
      "Recall               : 0.84593837535014\n",
      "F1 Score             : 0.8555240793201132\n",
      "Hamming Loss         : 0.021725239616613417\n",
      "Jaccard Score\n",
      "Anak                 : 0.8974358974358975\n",
      "Bedah                : 0.7941176470588235\n",
      "Gigi                 : 0.8571428571428571\n",
      "Gizi                 : 0.7037037037037037\n",
      "Jantung              : 0.2\n",
      "Jiwa                 : 0.6923076923076923\n",
      "Kandungan            : 0.803030303030303\n",
      "Kulit dan Kelamin    : 0.8913043478260869\n",
      "Mata                 : 0.875\n",
      "Paru                 : 1.0\n",
      "Penyakit Dalam       : 0.5555555555555556\n",
      "Saraf                : 0.42857142857142855\n",
      "THT                  : 0.90625\n",
      "Tulang               : 0.8181818181818182\n",
      "Urologi              : 0.4444444444444444\n",
      "Jaccard Score Macro Average: 0.7244697130172408\n",
      "Jaccard Score Micro Average: 0.7475247524752475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(transformer, generator, discriminator, dataset['Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f1aecf8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 340/340 [00:10<00:00, 31.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Classification Report ================\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "             Anak       0.79      0.66      0.72       286\n",
      "            Bedah       0.72      0.35      0.47       237\n",
      "             Gigi       0.95      0.78      0.86       232\n",
      "             Gizi       0.80      0.71      0.75       240\n",
      "          Jantung       0.71      0.44      0.55        72\n",
      "             Jiwa       0.76      0.63      0.69       144\n",
      "        Kandungan       0.79      0.81      0.80       393\n",
      "Kulit dan Kelamin       0.74      0.69      0.72       419\n",
      "             Mata       0.97      0.91      0.94       215\n",
      "             Paru       0.23      0.58      0.33        19\n",
      "   Penyakit Dalam       0.49      0.64      0.55       426\n",
      "            Saraf       0.38      0.25      0.30        95\n",
      "              THT       0.70      0.72      0.71       235\n",
      "           Tulang       0.82      0.53      0.65       132\n",
      "          Urologi       0.59      0.19      0.29       126\n",
      "\n",
      "        micro avg       0.72      0.65      0.68      3271\n",
      "        macro avg       0.70      0.59      0.62      3271\n",
      "     weighted avg       0.73      0.65      0.68      3271\n",
      "      samples avg       0.74      0.70      0.70      3271\n",
      "\n",
      "================ Multi Label Score ================\n",
      "Accuracy             : 0.5881486934118513\n",
      "Precision            : 0.7182526244497122\n",
      "Recall               : 0.6484255579333538\n",
      "F1 Score             : 0.6815552699228793\n",
      "Hamming Loss         : 0.04863206968470126\n",
      "Jaccard Score\n",
      "Anak                 : 0.5621301775147929\n",
      "Bedah                : 0.30855018587360594\n",
      "Gigi                 : 0.7551867219917012\n",
      "Gizi                 : 0.6063829787234043\n",
      "Jantung              : 0.3764705882352941\n",
      "Jiwa                 : 0.5290697674418605\n",
      "Kandungan            : 0.6652719665271967\n",
      "Kulit dan Kelamin    : 0.5579150579150579\n",
      "Mata                 : 0.8868778280542986\n",
      "Paru                 : 0.19642857142857142\n",
      "Penyakit Dalam       : 0.38115330520393814\n",
      "Saraf                : 0.1791044776119403\n",
      "THT                  : 0.5487012987012987\n",
      "Tulang               : 0.47619047619047616\n",
      "Urologi              : 0.16783216783216784\n",
      "Jaccard Score Macro Average: 0.4798177046163736\n",
      "Jaccard Score Micro Average: 0.5169388252498172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(transformer, generator, discriminator, dataset['Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9247ced",
   "metadata": {},
   "source": [
    "# ASO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21464882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aso_pred(transformer, generator, discriminator, instance):\n",
    "    transformer.eval() \n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Untuk setiap batch di validation data\n",
    "    pbar = tqdm(enumerate(instance), desc=\"Evaluate\", total=len(instance))\n",
    "    for step, batch in pbar:\n",
    "\n",
    "        # Unpack sebuah batch dari DataLoader\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_input_att_mask = batch[1].to(device)\n",
    "        batch_label = batch[2].to(device)\n",
    "        batch_mask = batch[3].to(device)\n",
    "        real_batch_size = batch_input_ids.shape[0]\n",
    "\n",
    "        # jangan track gradient! ini sedang evaluasi, bukan training\n",
    "        with torch.no_grad():        \n",
    "            model_outputs = transformer(batch_input_ids, attention_mask=batch_input_att_mask)\n",
    "            hidden_states = model_outputs[-1]\n",
    "            _, probs = discriminator(hidden_states)\n",
    "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "            filtered_logits = probs[:,0:-1]\n",
    "            # Accumulate the test loss.\n",
    "\n",
    "        pred_l = []\n",
    "        for prob in filtered_logits:\n",
    "            pred = [1 if x >= 0.5 else 0 for x in prob]\n",
    "            if (np.sum(pred) == 0):\n",
    "                _, idx = torch.max(prob, dim = 0)\n",
    "                pred[idx] = 1\n",
    "            pred_l.append(pred)\n",
    "        pred_labels += pred_l\n",
    "        true_l = batch_label\n",
    "        true_labels += true_l.detach().cpu()\n",
    "\n",
    "\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    true_labels = torch.stack(true_labels).numpy()\n",
    "    return pred_labels, true_labels\n",
    "\n",
    "def aso_eval(prediction, y_test):\n",
    "    return accuracy_score(y_test, prediction), f1_score(y_test, prediction, average='micro', zero_division=0), hamming_loss(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14a78925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada\n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import os\n",
    "\n",
    "# Specifying path\n",
    "\n",
    "path = 'Result/Deep_Learning_Result.csv'\n",
    "\n",
    "# Checking whether the specified path exists\n",
    "\n",
    "if not os.path.exists(path):\n",
    "  print(\"gada\")\n",
    "  f = open(path, \"w+\")\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerow(['Id', \"Learning Method\", 'Model', 'Accuracy', 'F1-Score', 'Hamming Loss'])\n",
    "  f.close()\n",
    "else:\n",
    "  print('ada')\n",
    "\n",
    "def record_result(row):\n",
    "  f = open(path, \"a\")\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerow(row)\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48177a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014178991317749023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 1340657204,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3349c631eb7490290fa58c16103ff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:10<00:00,  3.60it/s, d_loss=5.84, d_loss_sup=4.87, d_loss_unsup=0.98, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:05<00:00,  6.90it/s, loss=3.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25559105431309903, Precision (micro): 0.3035143769968051, Recall (micro): 0.2661064425770308\n",
      "Epoch 2 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:58<00:00,  3.02it/s, d_loss=2.97, d_loss_sup=2.22, d_loss_unsup=0.75, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:07<00:00,  5.59it/s, loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7252396166134185, Precision (micro): 0.8466453674121406, Recall (micro): 0.742296918767507\n",
      "Epoch 3 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:57<00:00,  3.03it/s, d_loss=1.92, d_loss_sup=1.18, d_loss_unsup=0.73, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:06<00:00,  6.19it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7348242811501597, Precision (micro): 0.8417721518987342, Recall (micro): 0.7450980392156863\n",
      "Epoch 4 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:56<00:00,  3.04it/s, d_loss=1.54, d_loss_sup=0.82, d_loss_unsup=0.73, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:07<00:00,  5.56it/s, loss=1.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7476038338658147, Precision (micro): 0.8493975903614458, Recall (micro): 0.7899159663865546\n",
      "Epoch 5 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:57<00:00,  3.03it/s, d_loss=1.30, d_loss_sup=0.58, d_loss_unsup=0.72, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:06<00:00,  6.22it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.744408945686901, Precision (micro): 0.8449848024316109, Recall (micro): 0.7787114845938375\n",
      "Epoch 6 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:59<00:00,  3.01it/s, d_loss=1.20, d_loss_sup=0.48, d_loss_unsup=0.72, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:06<00:00,  5.87it/s, loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7348242811501597, Precision (micro): 0.829971181556196, Recall (micro): 0.8067226890756303\n",
      "Epoch 7 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [04:57<00:00,  3.02it/s, d_loss=1.11, d_loss_sup=0.40, d_loss_unsup=0.72, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:07<00:00,  5.60it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7603833865814696, Precision (micro): 0.855457227138643, Recall (micro): 0.8123249299719888\n",
      "Epoch 8 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 94/901 [00:31<04:40,  2.87it/s, d_loss=0.10, d_loss_sup=0.03, d_loss_unsup=0.08, g_loss=0.08]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Train:  99%|█████████▊| 888/901 [04:54<00:04,  2.90it/s, d_loss=0.99, d_loss_sup=0.28, d_loss_unsup=0.70, g_loss=0.74]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformer, generator, discriminator = create_model(transformer, generator, discriminator)\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "num_train_instances = len(train_dataloader)\n",
    "num_train_steps = int(num_train_instances / BATCH_SIZE * EPOCH)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "#scheduler\n",
    "dis_scheduler = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "gen_scheduler = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "model = train_model(transformer, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler, dis_scheduler, num_epochs=EPOCH, last_epoch=0, last_acc=0)\n",
    "\n",
    "y_pred, y_test = aso_pred(transformer, generator, discriminator, dataset['Test'])\n",
    "acc, f1, hamm = aso_eval(y_pred, y_test)\n",
    "\n",
    "record_result([f\"gan-bert-1\", \"semi-supervised\", \"gan-bert\", acc, f1, hamm])\n",
    "\n",
    "evaluate_dataset(transformer, generator, discriminator, dataset['Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcae03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "transformer, generator, discriminator = create_model(transformer, generator, discriminator)\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator, weight_decay = REGULARIZATION)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator, weight_decay = REGULARIZATION) \n",
    "\n",
    "num_train_instances = len(train_dataloader)\n",
    "num_train_steps = int(num_train_instances / BATCH_SIZE * EPOCH)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "#scheduler\n",
    "dis_scheduler = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "gen_scheduler = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "model = train_model(transformer, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler, dis_scheduler, num_epochs=EPOCH, last_epoch=0, last_acc=0)\n",
    "\n",
    "y_pred, y_test = aso_pred(transformer, generator, discriminator, dataset['Test'])\n",
    "acc, f1, hamm = aso_eval(y_pred, y_test)\n",
    "\n",
    "record_result([f\"gan-bert-2\", \"semi-supervised\", \"gan-bert\", acc, f1, hamm])\n",
    "\n",
    "evaluate_dataset(transformer, generator, discriminator, dataset['Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a1ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "transformer, generator, discriminator = create_model(transformer, generator, discriminator)\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator, weight_decay = REGULARIZATION)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator, weight_decay = REGULARIZATION) \n",
    "\n",
    "num_train_instances = len(train_dataloader)\n",
    "num_train_steps = int(num_train_instances / BATCH_SIZE * EPOCH)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "#scheduler\n",
    "dis_scheduler = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "gen_scheduler = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "model = train_model(transformer, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler, dis_scheduler, num_epochs=EPOCH, last_epoch=0, last_acc=0)\n",
    "\n",
    "y_pred, y_test = aso_pred(transformer, generator, discriminator, dataset['Test'])\n",
    "acc, f1, hamm = aso_eval(y_pred, y_test)\n",
    "\n",
    "record_result([f\"gan-bert-3\", \"semi-supervised\", \"gan-bert\", acc, f1, hamm])\n",
    "\n",
    "evaluate_dataset(transformer, generator, discriminator, dataset['Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330f961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [06:19<00:00,  2.38it/s, d_loss=5.83, d_loss_sup=4.86, d_loss_unsup=0.97, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:10<00:00,  3.92it/s, loss=3.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2523961661341853, Precision (micro): 0.29073482428115016, Recall (micro): 0.2549019607843137\n",
      "Epoch 2 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:39<00:00,  2.66it/s, d_loss=2.96, d_loss_sup=2.22, d_loss_unsup=0.75, g_loss=0.72]\n",
      "Val: 100%|██████████| 40/40 [00:10<00:00,  3.77it/s, loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7124600638977636, Precision (micro): 0.8370607028753994, Recall (micro): 0.7338935574229691\n",
      "Epoch 3 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:40<00:00,  2.65it/s, d_loss=1.91, d_loss_sup=1.18, d_loss_unsup=0.73, g_loss=0.73]\n",
      "Val: 100%|██████████| 40/40 [00:10<00:00,  3.78it/s, loss=1.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7380191693290735, Precision (micro): 0.8575949367088608, Recall (micro): 0.7591036414565826\n",
      "Epoch 4 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:40<00:00,  2.64it/s, d_loss=1.53, d_loss_sup=0.81, d_loss_unsup=0.73, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:10<00:00,  3.83it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7156549520766773, Precision (micro): 0.824773413897281, Recall (micro): 0.7647058823529411\n",
      "Epoch 5 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:39<00:00,  2.66it/s, d_loss=1.34, d_loss_sup=0.62, d_loss_unsup=0.72, g_loss=0.74]\n",
      "Val: 100%|██████████| 40/40 [00:10<00:00,  3.79it/s, loss=1.10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7603833865814696, Precision (micro): 0.8650306748466258, Recall (micro): 0.7899159663865546\n",
      "Epoch 6 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:39<00:00,  2.66it/s, d_loss=1.23, d_loss_sup=0.51, d_loss_unsup=0.72, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:10<00:00,  3.78it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7667731629392971, Precision (micro): 0.8501440922190202, Recall (micro): 0.8263305322128851\n",
      "Epoch 7 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:38<00:00,  2.66it/s, d_loss=1.11, d_loss_sup=0.39, d_loss_unsup=0.72, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:10<00:00,  3.81it/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7891373801916933, Precision (micro): 0.878698224852071, Recall (micro): 0.8319327731092437\n",
      "Epoch 8 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:38<00:00,  2.66it/s, d_loss=1.04, d_loss_sup=0.32, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:10<00:00,  3.78it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7603833865814696, Precision (micro): 0.8448753462603878, Recall (micro): 0.8543417366946778\n",
      "Epoch 9 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:40<00:00,  2.65it/s, d_loss=0.98, d_loss_sup=0.27, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:09<00:00,  4.35it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7795527156549521, Precision (micro): 0.861271676300578, Recall (micro): 0.834733893557423\n",
      "Epoch 10 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 901/901 [05:41<00:00,  2.64it/s, d_loss=0.91, d_loss_sup=0.20, d_loss_unsup=0.71, g_loss=0.75]\n",
      "Val: 100%|██████████| 40/40 [00:08<00:00,  4.66it/s, loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7571884984025559, Precision (micro): 0.8526011560693642, Recall (micro): 0.8263305322128851\n",
      "Epoch 11 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  73%|███████▎  | 662/901 [04:09<01:28,  2.70it/s, d_loss=0.64, d_loss_sup=0.11, d_loss_unsup=0.52, g_loss=0.55]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformer, generator, discriminator = create_model(transformer, generator, discriminator)\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator, weight_decay = REGULARIZATION)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator, weight_decay = REGULARIZATION) \n",
    "\n",
    "num_train_instances = len(train_dataloader)\n",
    "num_train_steps = int(num_train_instances / BATCH_SIZE * EPOCH)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "#scheduler\n",
    "dis_scheduler = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "gen_scheduler = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "model = train_model(transformer, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler, dis_scheduler, num_epochs=EPOCH, last_epoch=0, last_acc=0)\n",
    "\n",
    "y_pred, y_test = aso_pred(transformer, generator, discriminator, dataset['Test'])\n",
    "acc, f1, hamm = aso_eval(y_pred, y_test)\n",
    "\n",
    "record_result([f\"gan-bert-4\", \"semi-supervised\", \"gan-bert\", acc, f1, hamm])\n",
    "\n",
    "evaluate_dataset(transformer, generator, discriminator, dataset['Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46054410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "transformer, generator, discriminator = create_model(transformer, generator, discriminator)\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator, weight_decay = REGULARIZATION)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator, weight_decay = REGULARIZATION) \n",
    "\n",
    "num_train_instances = len(train_dataloader)\n",
    "num_train_steps = int(num_train_instances / BATCH_SIZE * EPOCH)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "#scheduler\n",
    "dis_scheduler = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "gen_scheduler = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "model = train_model(transformer, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler, dis_scheduler, num_epochs=EPOCH, last_epoch=0, last_acc=0)\n",
    "\n",
    "y_pred, y_test = aso_pred(transformer, generator, discriminator, dataset['Test'])\n",
    "acc, f1, hamm = aso_eval(y_pred, y_test)\n",
    "\n",
    "record_result([f\"gan-bert-5\", \"semi-supervised\", \"gan-bert\", acc, f1, hamm])\n",
    "\n",
    "evaluate_dataset(transformer, generator, discriminator, dataset['Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal\n",
    "\n",
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370a184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
